<!DOCTYPE html>

<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Yuzhe Gu's home page"> 
  
  <link href="./files/wfdoc.css" rel="stylesheet" type="text/css"> 
  <title>Yuzhe Gu's Homepage</title> 
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Yuzhe Gu &nbsp; 顾宇喆</h1></div>
        <h3>PhD Student</h3>
        <p>Large Model Center, Shanghai AI Laboratory
        <br>School of Electronic Information and Electrical Engineering,
        <br>Shanghai Jiao Tong University
        <br><br>
        Email: guyuzhe@pjlab.org.cn, guyuzhe1116@sjtu.edu.cn;
        <br>
        Google Scholar: <a href="https://scholar.google.com/citations?user=NaiWQ5oAAAAJ"> Google Scholar Link</a>;
        Github: <a href="https://github.com/Liqu1d-G"> Github Link</a> 
        <br><br></p>
      </td>
      <td>
        <div>
          <img width="150" src="./files/gyz.jpeg" border="0">
          <!-- <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> -->
        </div>
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">


  <h2>Biography</h2>
  <p> I am a first-year PhD student at Shanghai Jiao Tong University, in the joint program at Shanghai AI Laboratory, advised by <a href="https://zhangwenwei.cn/">Wenwei Zhang</a> and <a href="https://chenkai.site/">Kai Chen</a>. Before that, I received the bachelor degree at Wuhan University in 2024.
  </p>
  <p> My research interests lie primarily in the area of Large Language Model (LLM). I'm focusing on improving the reasoning and knowledge capabilities of LLMs.  I also have experience about the reducing hallucination in LLMs, including the annotation, detection and mitigation of hallucinations. </p>
  <p>Discussions and cooperations are welcomed! </p>

  <h2>News</h2>
  <ul>
    <li>
      [2025.01] Our paper <a href="">Mask-DPO</a> is accepted by ICLR 2025.
    </li>
    <li>
      [2024.12] We release <a href="https://internlm-chat.intern-ai.org.cn">InternThinker</a>, a powerful reasoning model.
    </li>
    <li>
      [2024.09] Our paper <a href="https://arxiv.org/abs/2407.04693">ANAH-v2</a> is accepted by NeurIPS 2024.
    </li>
    <li>
      [2024.05] Our paper <a href="https://arxiv.org/abs/2405.20315">ANAH</a> is accepted by ACL 2024.
    </li>
  </ul>


  <h2>Publications</h2>
  * denotes equal contribution. 

  <table class="pub_table">
  <tbody>
    <h3>(Co-) First author Papers</h3>
    <table class="pub_table">
      <tbody>

      <tr>
        <td class="pub_td1"><img src="./files/oreal.jpg" class="papericon"></td>
        <td class="pub_td2"><b>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning
        </b>
          <br>Chengqi Lyu*, Songyang Gao*, <b>Yuzhe Gu*</b>, Wenwei Zhang*, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, Kai Chen
          <br><i>preprint</i>
          <br>
          [<a href="https://arxiv.org/abs/2502.06781">Paper</a>]
          [<a href="https://github.com/InternLM/OREAL">Code</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="./files/maskdpo.jpg" class="papericon"></td>
        <td class="pub_td2"><b>Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs
        </b>
          <br><b>Yuzhe Gu</b>, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen
          <br><i>The Thirteenth International Conference on Learning Representations (<b>ICLR</b>) , 2025</i>
          <br>
          [<a href="https://arxiv.org/abs/2503.02846">Paper</a>]
          [<a href="https://github.com/open-compass/ANAH">Code</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="./files/anahv2.png" class="papericon"></td>
        <td class="pub_td2"><b>ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models
        </b>
          <br><b>Yuzhe Gu*</b>, Ziwei Ji*, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen
          <br><i>The Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>) , 2024</i>
          <br>
          [<a href="https://arxiv.org/abs/2407.04693">Paper</a>]
          [<a href="https://github.com/open-compass/ANAH">Code</a>]
          [<a href="https://open-compass.github.io/ANAH/">Project</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="./files/anah.jpg" class="papericon"></td>
        <td class="pub_td2"><b>ANAH: Analytical Annotation of Hallucinations in Large Language Models
        </b>
          <br>Ziwei Ji*, <b>Yuzhe Gu*</b>, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen
          <br><i>The 62nd Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>) , 2024</i>
          <br>
          [<a href="https://arxiv.org/abs/2405.20315">Paper</a>]
          [<a href="https://github.com/open-compass/ANAH">Code</a>]
          [<a href="https://open-compass.github.io/ANAH/">Project</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="./files/extendcache.jpg" class="papericon"></td>
        <td class="pub_td2"><b>One more set: Mitigating conflict-based cache side-channel attacks by extending cache set
        </b>
          <br><b>Yuzhe Gu</b>, Ming Tang, Quancheng Wang, Han Wang, Haili Ding
          <br><i>Journal of Systems Architecture (<b>JSA</b>) </i>
          <br>
          [<a href="https://www.sciencedirect.com/science/article/pii/S1383762123001765">Paper</a>]
        </td>
      </tr>

    </tbody>
    </table>

    <h3>Co-author Papers</h3>
    <table class="pub_table">
      <tbody>

      <tr>
        <td class="pub_td1"><img src="./files/backcache.jpg" class="papericon"></td>
        <td class="pub_td2"><b>BackCache: Mitigating contention-based cache timing attacks by hiding cache line evictions</b>
          <br>Quancheng Wang, Xige Zhang, Han Wang, <b>Yuzhe Gu</b>, Ming Tang
          <br><i>arXiv 2023, Under Review</i>
          <br>
          [<a href="https://arxiv.org/pdf/2304.10268">Paper</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="./files/sage.jpg" class="papericon"></td>
        <td class="pub_td2"><b>Redeem myself: Purifying backdoors in deep learning models using self attention distillation
        </b>
          <br>Xueluan Gong, Yanjiao Chen, Wang Yang, Qian Wang, <b>Yuzhe Gu</b>, Huayang Huang, Chao Shen
          <br><i> 44th IEEE Symposium on Security and Privacy (<b>Oakland</b>), 2023</i>
          <br>
          [<a href="https://ieeexplore.ieee.org/abstract/document/10179375/">Paper</a>]
        </td>
      </tr>

      </tbody>
    </table>

  <h2>Projects</h2>
  <ul>
    <li>
      <a href="https://internlm-chat.intern-ai.org.cn">InternThinker</a>: a powerful reasoning model.
    </li>
    <li>
      <a href="https://github.com/InternLM/lagent">Lagent</a>: a lightweight open-source framework that allows users to efficiently build LLM-based agents
    </li>
    <li>
      <a href="https://github.com/InternLM/InternLM">InternLM</a>: state-of-the-art open-source LLMs varying from 7B to 123B.
    </li>
  </ul>

  <h2>Awards</h2>
  <ul>
    <li>
      Outstanding Undergraduate of Wuhan University, 2024
    </li>
    <li>
      <a href="https://mp.weixin.qq.com/s/LH7eHxbsRryGU-2nQ9hYjA">Lei Jun Excellence Scholarship</a> of Wuhan University, 2024 (10w RMB)
    </li>
    <li>
      First Prize Excellence Scholarship of Wuhan University, 2021, 2022, 2023
    </li>
    
  </ul>

</div>
</div>

</body></html>
